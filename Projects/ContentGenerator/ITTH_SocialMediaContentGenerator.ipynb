{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ‚úçÔ∏è **Multi-Agent Social Media Content Generator**\n",
        "\n",
        "## **Project Overview**\n",
        "\n",
        "This project implements a sophisticated, multi-platform social media content generator using an **agent-based architecture** and advanced **Large Language Models** (LLMs). The goal is to produce tailored, platform-specific content (Twitter/X, Instagram, Facebook, LinkedIn, and Hashtags) from a single user input (Topic, Tone, and Language), ensuring **multilingual output** and strict adherence to platform-specific rules and professional validation requirements.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## üõ†Ô∏è **Key Technologies & Frameworks**\n",
        "\n",
        "| Technology | Role in Project |\n",
        "| :--- | :--- |\n",
        "| **Gradio** | Provides the simple, intuitive web interface (UI) for user input and displaying generated content. |\n",
        "| **LangChain** | Used to build and manage the **LLM chains**, including prompt templating, output parsing, and integrating structured data models. |\n",
        "| **Groq API** | The high-speed LLM service (`llama-3.1-8b-instant`) used to perform all the generative and classification tasks. |\n",
        "| **Pydantic** | Used by LangChain for **Structured Output** to reliably classify the topic's relevance for LinkedIn. |\n",
        "| **Python OOP**| Implemented via **Dedicated Agent Classes** (e.g., `TwitterAgent`, `LinkedInAgent`) for clean, maintainable, and modular code design. |\n",
        "\n",
        "---\n",
        "\n",
        "## ‚öôÔ∏è **Core Functionality and Agent Rules**\n",
        "\n",
        "The project features a **`ContentRunner`** orchestration class that delegates tasks to specialized agents. Each agent is pre-programmed with platform-specific constraints, ensuring high-quality, formatted output.\n",
        "\n",
        "| Platform Agent | Specific Rule Enforced |\n",
        "| :--- | :--- |\n",
        "| **Twitter/X Agent** | A single, highly engaging, attention-grabbing hook (under 280 characters). |\n",
        "| **Instagram Agent** | A fun, two-paragraph caption, separated by a double line break. |\n",
        "| **Facebook Agent** | A friendly, conversational post (2-3 sentences). |\n",
        "| **LinkedIn Agent** | A professional, insightful summary (3-4 sentences). |\n",
        "| **Hashtag Agent** | Exactly 3 relevant, highly specific hashtags. |\n",
        "\n",
        "### üíº **Critical Validation Rule (LinkedIn)**\n",
        "\n",
        "The runner includes a crucial Pydantic-driven **Topic Classification** step to enforce professional standards:\n",
        "\n",
        "* The LinkedIn post is generated **only** if the input topic is classified as **business, tech, finance, or career relevant**.\n",
        "* If the topic is *not* relevant (e.g., purely recreational), the LinkedIn output field is returned as an **empty string** (`\"\"`).\n",
        "\n",
        "---\n",
        "\n",
        "## üîë **Setup and Prerequisites**\n",
        "\n",
        "To run this notebook, you **MUST** obtain and set your `GROQ_API_KEY`. This key is essential for the `ChatGroq` class to communicate with the Large Language Model service.\n",
        "\n",
        "1.  **Install Libraries:** Uncomment and run the `!pip install` command block.\n",
        "2.  **Set API Key:** Replace the `YOUR_GROQ_API_KEY_HERE` placeholder in the code or set the `GROQ_API_KEY` as a Colab secret/environment variable."
      ],
      "metadata": {
        "id": "ijEoxiJ0v4g3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Initial Setup**"
      ],
      "metadata": {
        "id": "-mF0eGZ1wOxY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# Installation (Uncomment and run if libraries are missing in your environment)\n",
        "#\n",
        "\n",
        "#\n",
        "# The option -q of pip give less output.\n",
        "#\n",
        "# The Option is additive. In other words, you can use it up to 3 times (corresponding to WARNING, ERROR, and CRITICAL logging levels).\n",
        "#\n",
        "# So:\n",
        "#   -q   means display only the messages with WARNING,ERROR,CRITICAL log levels\n",
        "#   -qq  means display only the messages with ERROR,CRITICAL log levels\n",
        "#   -qqq means display only the messages with CRITICAL log level\n",
        "#\n",
        "# !pip install -qqq gradio langchain-groq langchain_core pydantic"
      ],
      "metadata": {
        "id": "WLLKYdRVwk9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UNngzVixtk3w"
      },
      "outputs": [],
      "source": [
        "# #\n",
        "# # Import Libraries\n",
        "# #\n",
        "# import os                  # Used to retrieve the GROQ_API_KEY from the system environment variables.\n",
        "# import gradio as gr        # THE UI LIBRARY: Used to create the web interface (Inputs, Buttons, Outputs) quickly.\n",
        "# from typing import Literal # Used for type hinting, specifically to limit a variable's value to a specific set of strings.\n",
        "\n",
        "# #\n",
        "# # NOTE\n",
        "# # While these are imported, they are not strictly used when calling a remote API like Groq.\n",
        "# #\n",
        "# import transformers # Core library for working with models (often used for tokenization/local models).\n",
        "# import torch        # The leading deep learning framework, often used for model computation.\n",
        "# import accelerate   # A library to easily run large model training/inference across different hardware (GPUs).\n",
        "\n",
        "# #\n",
        "# # LangChain Imports - Core libraries for building complex LLM applications\n",
        "# #\n",
        "# from langchain_groq import ChatGroq                       # The specific connector (wrapper) class for the Groq API.\n",
        "# from langchain_core.prompts import ChatPromptTemplate     # The foundation for creating structured, multi-role prompts.\n",
        "# from langchain_core.output_parsers import StrOutputParser # A parser to simply return the LLM's response as a basic string.\n",
        "\n",
        "# # from langchain_core.pydantic_v1 import BaseModel, Field   # Pydantic classes for reliable, structured data handling.\n",
        "# # WARNING FIX: Updated import to eliminate the LangChainDeprecationWarning.\n",
        "# # We now import Pydantic components directly from the Pydantic library.\n",
        "\n",
        "# from pydantic import BaseModel, Field                     # Pydantic classes for reliable, structured data handling."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# Import Common Libraries\n",
        "#\n",
        "import os         # Used to retrieve the GROQ_API_KEY from the system environment variables.\n",
        "import sys        # Used for environment detenction and exiting the script\n",
        "import subprocess # For dependency installantion to work from command line (!pip raises syntax error at compile time)\n",
        "\n",
        "#\n",
        "# Conditional Dependency Installation for Notebook and Script Compatibility\n",
        "#\n",
        "try:\n",
        "  import gradio as gr        # THE UI LIBRARY : Used to create the web interface (Inputs, Buttons, Outputs) quickly.\n",
        "  from typing import Literal # Used for type hinting, specifically to limit a variable's value to a specific set of strings.\n",
        "\n",
        "  #\n",
        "  # NOTE\n",
        "  # While these are imported, they are not strictly used when calling a remote API like Groq.\n",
        "  #\n",
        "  import transformers # Core library for working with models (often used for tokenization/local models).\n",
        "  import torch        # The leading deep learning framework, often used for model computation.\n",
        "  import accelerate   # A library to easily run large model training/inference across different hardware (GPUs).\n",
        "\n",
        "  #\n",
        "  # LangChain Imports - Core libraries for building complex LLM applications\n",
        "  #\n",
        "  from langchain_groq import ChatGroq                       # The specific connector (wrapper) class for the Groq API.\n",
        "  from langchain_core.prompts import ChatPromptTemplate     # The foundation for creating structured, multi-role prompts.\n",
        "  from langchain_core.output_parsers import StrOutputParser # A parser to simply return the LLM's response as a basic string.\n",
        "\n",
        "  # from langchain_core.pydantic_v1 import BaseModel, Field   # Pydantic classes for reliable, structured data handling.\n",
        "  # WARNING FIX: Updated import to eliminate the LangChainDeprecationWarning.\n",
        "  # We now import Pydantic components directly from the Pydantic library.\n",
        "\n",
        "  from pydantic import BaseModel, Field                     # Pydantic classes for reliable, structured data handling.\n",
        "except ImportError:\n",
        "    #\n",
        "    # This block executes ONLY if the initial imports fail\n",
        "    # (i.e., dependencies are missing)\n",
        "    #\n",
        "    print(\"Dependencies not found. Attempting to install...\")\n",
        "\n",
        "    #\n",
        "    # Check if running in an interactive notebook environment (Colab/Jupyter)\n",
        "    #\n",
        "    if '__package__' in globals() or 'ipykernel' in sys.modules:\n",
        "      print(\"Notebook environment detected. Using subprocess.check_call...\")\n",
        "\n",
        "      #\n",
        "      # Use notebook-specific command for quiet installation\n",
        "      #\n",
        "      # !pip install -qqq gradio langchain-groq langchain_core pydantic\n",
        "      #\n",
        "      # Use subprocess to run pip, which works in both notebook and\n",
        "      # script environments\n",
        "      #\n",
        "      subprocess.check_call(\n",
        "                             [\n",
        "                               sys.executable,\n",
        "                               \"-m\",\n",
        "                               \"pip\",\n",
        "                               \"install\",\n",
        "                               \"-qqq\",\n",
        "                               \"gradio\",\n",
        "                               \"langchain-groq\",\n",
        "                               \"langchain_core\",\n",
        "                               \"pydantic\",\n",
        "                               \"transformers\",\n",
        "                               \"torch\",\n",
        "                               \"accelerate\"\n",
        "                             ]\n",
        "                           )\n",
        "    else:\n",
        "      print(\"\")\n",
        "      print(\"**Dependencies are missing.**\")\n",
        "      print(\"Please install required libraries manually using:\")\n",
        "      print(\"pip install gradio langchain-groq langchain_core pydantic transformers torch accelerate\")\n",
        "\n",
        "      #\n",
        "      # Exit if dependencies are missing in a script environment\n",
        "      #\n",
        "      sys.exit(1)\n",
        "\n",
        "    # Re-import after successful installation (necessary for notebooks)\n",
        "    try:\n",
        "      import os\n",
        "      import sys\n",
        "      import gradio as gr\n",
        "      from typing import Literal\n",
        "\n",
        "      #\n",
        "      # NOTE\n",
        "      # While these are imported, they are not strictly used when calling a remote API like Groq.\n",
        "      #\n",
        "      import transformers\n",
        "      import torch\n",
        "      import accelerate\n",
        "\n",
        "      from langchain_groq import ChatGroq\n",
        "      from langchain_core.prompts import ChatPromptTemplate\n",
        "      from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "      from pydantic import BaseModel, Field\n",
        "    except Exception as e:\n",
        "      print(f\"Failed to load dependencies after installation: {e}\")\n",
        "      sys.exit(1)"
      ],
      "metadata": {
        "id": "FGvkR1tbUbT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# Wrapper to run the code in an non-google colab environment\n",
        "# This block ensures the code can access secrets/environment variables\n",
        "# whether running in Google Colab or a standard Python environment\n",
        "#\n",
        "try:\n",
        "  from google.colab import userdata\n",
        "  #\n",
        "  # If running in Google Colab, the 'userdata' object is successfully\n",
        "  # imported.\n",
        "  # It is used to securely fetch secrets from the Colab Secrets Manager\n",
        "  #\n",
        "except ImportError:\n",
        "  #\n",
        "  # Dummy class for non-Colab environments\n",
        "  #\n",
        "  # If an ImportError occurs (meaning we are NOT in Colab), this 'except' block\n",
        "  # executes to define a fallback mechanism\n",
        "  #\n",
        "  class DummyUserdata:\n",
        "    #\n",
        "    # The 'get' method is implemented to mimic the Colab 'userdata.get()'\n",
        "    # functionality, allowing the rest of the code to remain the same\n",
        "    #\n",
        "    def get(self, name):\n",
        "      #\n",
        "      # Instead of using the Colab Secrets Manager, this method looks for the\n",
        "      # variable (name) in the operating system's standard environment\n",
        "      # variables using 'os.getenv()'\n",
        "      #\n",
        "      return os.getenv(name)\n",
        "\n",
        "  #\n",
        "  # An instance of the dummy class is created and assigned to 'userdata'\n",
        "  # This makes 'userdata.get(name)' work correctly in both environments\n",
        "  #\n",
        "  userdata = DummyUserdata()"
      ],
      "metadata": {
        "id": "FMVoCYgfzOHH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Configuration**"
      ],
      "metadata": {
        "id": "0QL-bbo313ed"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SECRET_NAME     = \"GROQ_API_KEY\"\n",
        "UNKNOWN_API_KEY = \"UNKNOWN_API_KEY\"\n",
        "GROQ_MODEL      = \"llama-3.1-8b-instant\" # \"llama3-8b-8192\" # OLD, DECOMMISSIONED MODEL (Causing the Error)\n",
        "\n",
        "SHARE           = True\n",
        "NO_SHARE        = False"
      ],
      "metadata": {
        "id": "WXv07GdA2HCP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# Fetch API Key Value\n",
        "#\n",
        "# Make it more generic by setting an OS Environment Variable\n",
        "# after fetching from Google Colab\n",
        "#\n",
        "\n",
        "#\n",
        "# Retrieve the API key securely from Colab Secrets.\n",
        "#\n",
        "try:\n",
        "  API_KEY = userdata.get(SECRET_NAME)\n",
        "\n",
        "  #\n",
        "  # Set it as an environment variable so the rest of your script\n",
        "  # can access it using os.getenv()\n",
        "  #\n",
        "  if API_KEY:\n",
        "    os.environ[SECRET_NAME] = API_KEY\n",
        "    print(\"‚úÖ API Key successfully retrieved from Colab Secrets and set as environment variable.\")\n",
        "  else:\n",
        "    print(f\"‚ùå Error: Secret '{SECRET_NAME}' was found but is empty. Please check the value.\")\n",
        "except Exception as e:\n",
        "  print(f\"‚ùå Error retrieving Secret '{SECRET_NAME}'. Ensure it is saved correctly in the Colab sidebar.\")\n",
        "  print(f\"Detailed error: {e}\")\n",
        "\n",
        "#\n",
        "# Rest of the program, can now use\n",
        "#\n",
        "# This also ensures that the program works seemlessly in a non-google colab\n",
        "# environment\n",
        "#\n",
        "API_KEY = os.getenv(SECRET_NAME, UNKNOWN_API_KEY)\n",
        "print(f\"Key check (last 3 chars): ...{API_KEY[-3:] if API_KEY and len(API_KEY) > 3 else 'N/A'}\")"
      ],
      "metadata": {
        "id": "pyCVYHng12ry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Relevance Model (Pydantic Schema)**"
      ],
      "metadata": {
        "id": "Wv0WTeSv55Wo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# LinkedIn Relevance Pydantic Model and Classifier\n",
        "#\n",
        "class TopicClassification(BaseModel):\n",
        "  #\n",
        "  # CLASS: Pydantic Model\n",
        "  #   Pydantic is a Python library that uses Python type hints for data\n",
        "  #   validation, parsing, and settings management, making code more robust\n",
        "  #   and reliable.\n",
        "  #\n",
        "  #   The Ellipsis (...), one use below, means Mandatory\n",
        "  #\n",
        "  #   WHAT IT DOES  : Defines the EXACT structure of the JSON output we want\n",
        "  #                   from the LLM.\n",
        "  #   WHY WE USE IT : LangChain uses this schema to instruct the LLM to return\n",
        "  #                   a clean, reliable JSON object, which is crucial for\n",
        "  #                   decision-making (the LinkedIn rule)\n",
        "  #\n",
        "  isTechBizRelevant : Literal[\"Yes\", \"No\"] = Field(\n",
        "                                                    #\n",
        "                                                    # The field name the LLM must use in its JSON output.\n",
        "                                                    # Detail on '...' (Ellipsis):\n",
        "                                                    # In Pydantic, the Ellipsis '...' is often used as a special placeholder\n",
        "                                                    # to indicate that a field **MUST** be provided (it is REQUIRED) and has no default value.\n",
        "                                                    ...,\n",
        "                                                    # Type Hint: Forces the field value to be either the string \"Yes\" or the string \"No\".\n",
        "                                                    #\n",
        "\n",
        "                                                    #\n",
        "                                                    # This description is passed directly to the LLM to guide its classification output.\n",
        "                                                    #\n",
        "                                                    description = \"Must be 'Yes' if the topic is primarily about technology, business, finance, career development, or professional skills. Otherwise, respond 'No'.\"\n",
        "                                                  )"
      ],
      "metadata": {
        "id": "r-kjXgDK55x0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def classifyTopic(llm: ChatGroq, topic: str) -> str:\n",
        "  #\n",
        "  # Function : classifyTopic\n",
        "  #  Parameters\n",
        "  #  llm: ChatGroq -> Parameter llm of type ChatGroq\n",
        "  #  topic:str     -> Parameter topic of type str\n",
        "  #\n",
        "  #  -> str        -> Return a strin\n",
        "  #\n",
        "  #  WHAT IT DOES  : Determines if the user's topic is appropriate for a\n",
        "  #                  professional LinkedIn post.\n",
        "  #  WHY WE USE IT : This function enforces the validation rule:\n",
        "  #                  'LinkedIn post should be an empty string if the topic is not business/tech relevant'\n",
        "  #\n",
        "  #  RETURNS       : A string: either \"Yes\" or \"No\".\n",
        "  #\n",
        "\n",
        "  # LangChain Component Detail: ChatPromptTemplate.from_messages\n",
        "  #   WHAT IT DOES  : Creates a reusable, structured blueprint for the conversation.\n",
        "  #   WHY WE USE IT : LLMs work best with role-based input.\n",
        "  #                   This allows us to define:\n",
        "  #                     1. A 'system' role\n",
        "  #                        (telling the model *how* to behave: \"You are a specialized classifier\").\n",
        "  #                     2. A 'user' role\n",
        "  #                        (passing the variable input: the topic to classify).\n",
        "  #\n",
        "  # WHAT IT RETURNS  : A PromptTemplate object.\n",
        "  #\n",
        "\n",
        "  #\n",
        "  # CRITICAL FIX: Made the system prompt more robust by providing examples\n",
        "  #\n",
        "  classifierPrompt = ChatPromptTemplate.from_messages(\n",
        "                                                       [\n",
        "                                                         (\"system\", \"You are a specialized classifier. Determine if the topic is relevant to a professional audience. This includes **business strategy, human resources (HR), finance, technology, AI, career development, professional skills, and organizational policy**. Specifically, topics about **employee retention, productivity, work week structure (e.g., 4-day work week), remote work, leadership, and company policy** MUST be classified as 'Yes'. Respond ONLY with the strict JSON format defined.\"),\n",
        "                                                         (\"user\", f\"Classify the following topic: '{{topic}}'\") # {topic} is a variable filled later.\n",
        "                                                       ]\n",
        "                                                     )\n",
        "\n",
        "  # classifierPrompt = ChatPromptTemplate.from_messages(\n",
        "  #                                                      [\n",
        "  #                                                        (\"system\", \"You are a specialized classifier. Analyze the user's topic and determine if it is relevant to a professional audience (e.g., **business strategy, human resources, finance, career development, or professional skills**). Topics about **employee retention, productivity, or work policy** must be classified as 'Yes'. Respond only with the strict JSON format defined.\"),\n",
        "  #                                                        (\"user\", f\"Classify the following topic: '{{topic}}'\") # {topic} is a variable filled later.\n",
        "  #                                                      ]\n",
        "  #                                                    )\n",
        "\n",
        "  # classifierPrompt = ChatPromptTemplate.from_messages(\n",
        "  #                                                      [\n",
        "  #                                                        (\"system\", \"You are a specialized classifier. Analyze the user's topic and determine if it is relevant to a professional audience (business, technology, finance, career, professional skills). Respond only with the strict JSON format defined.\"),\n",
        "  #                                                        (\"user\", f\"Classify the following topic: '{{topic}}'\") # {topic} is a variable filled later.\n",
        "  #                                                      ]\n",
        "  #                                                    )\n",
        "\n",
        "  #\n",
        "  # Chain Construction: Prompt | LLM (with forced JSON output)\n",
        "  #\n",
        "  # llm.with_structured_output(TopicClassification): This is the magic! It forces the LLM to format its response\n",
        "  # STRICTLY according to the TopicClassification Pydantic model.\n",
        "  #\n",
        "  # --------------------------------------------------------\n",
        "  # LangChain Expression Language (LCEL) Chain Construction\n",
        "  # --------------------------------------------------------\n",
        "  #\n",
        "  # LCEL allows us to define a data flow pipeline using the pipe operator (|).\n",
        "  #\n",
        "  # The output of the left side is automatically fed as the input to the right side.\n",
        "  #\n",
        "  #   1. Left Side     : classifierPrompt (Output : Formatted Prompt)\n",
        "  #   2. Pipe Operator : | (Data Flow Sequencer)\n",
        "  #   3. Right Side    : llm.with_structured_output(TopicClassification)\n",
        "  #\n",
        "  # Detail on the Right Side (LLM Executor):\n",
        "  #   llm                                          : This is the instantiated ChatGroq model.\n",
        "  #   .with_structured_output(TopicClassification) : This method is the magic!\n",
        "  #\n",
        "  #   WHAT IT DOES  : It modifies the LLM's behavior by telling it to\n",
        "  #                   **force** its output into a JSON structure that matches\n",
        "  #                   the Pydantic model TopicClassification.\n",
        "  #   WHY WE USE IT : Ensures the output is clean, reliable JSON, preventing\n",
        "  #                   parsing errors.\n",
        "  #\n",
        "  classificationChain = classifierPrompt | llm.with_structured_output(TopicClassification)\n",
        "\n",
        "  try:\n",
        "    #\n",
        "    # Invoke the chain, passing the topic variable\n",
        "    #\n",
        "    classification = classificationChain.invoke({\"topic\" : topic})\n",
        "\n",
        "    #\n",
        "    # Access the structured Pydantic object's field and return its value\n",
        "    # (\"Yes\" or \"No\")\n",
        "    #\n",
        "    return classification.isTechBizRelevant\n",
        "  except Exception as e:\n",
        "    #\n",
        "    # Fallback for API or parsing errors, defaulting to safe/non-relevant to\n",
        "    # skip LinkedIn post\n",
        "    #\n",
        "    print(f\"Classification failed (Defaulting to 'No'): {e}\")\n",
        "    return \"No\""
      ],
      "metadata": {
        "id": "EA9B21aC8nEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Social Media Agents**"
      ],
      "metadata": {
        "id": "dylJ5OJPLUrF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Base Class and Implementation**"
      ],
      "metadata": {
        "id": "LkZp7qRXLaNg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Method Conventions**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**The Underscore Convention**  \n",
        "When a method or attribute name is prefixed with a **single underscore (`_`)**, it signals to other developers that the method is intended for **internal use** by the class and should not be directly called from outside the class.\n",
        "\n",
        "**In Python, methods prefixed with `_` are called \"protected\" members.** They are still technically accessible (you can call them, e.g., `runner._initializeLLM()`), but doing so violates the developer's intent and can lead to bugs if the class's internal logic changes later.\n",
        "\n",
        "**No Underscore Convention**  \n",
        "Methods without an underscore prefix are considered **public** members. They define the official **interface** or public contract of the class. These methods are safe and intended to be called directly by other parts of the program.\n",
        "\n",
        "In summary, the underscore is a way developers communicate **intent** about how their classes should be used: **Public** methods define what the class does, and **Private/Protected (`_`)** methods define how the class does it internally."
      ],
      "metadata": {
        "id": "eQzsNXZ_TKKG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# Base class defining the common structure for all social media agents\n",
        "#\n",
        "# WHAT IT DOES  : A template class that provides common setup (LLM, prompt) for\n",
        "#                 all agents\n",
        "# WHY WE USE IT : Reduces code duplication by having all agents inherit the\n",
        "#                 standard chain creation and generation logic.\n",
        "#                 This is the Object-Oriented Programming (OOP)\n",
        "#                 principle of Inheritance.\n",
        "#\n",
        "class baseAgent:\n",
        "  #\n",
        "  # Constructor method: Sets up the Agent's properties\n",
        "  #\n",
        "  def __init__(self, llm : ChatGroq, platform : str, rules : str):\n",
        "    self.platform = platform               # e.g., \"Twitter/X\"\n",
        "    self.rules    = rules                  # The specific rules (e.g., \"under 280 characters\")\n",
        "    self.chain    = self._createChain(llm) # Initializes the LangChain pipeline immediately\n",
        "\n",
        "  def _createChain(self, llm : ChatGroq):\n",
        "    #\n",
        "    # METHOD        : _create_chain (A factory method)\n",
        "    # WHAT IT DOES  : Builds the specific LangChain pipeline for this platform.\n",
        "    # WHY WE USE IT : Encapsulates the prompt definition logic.\n",
        "    # RETURNS       : A LangChain Runnable sequence (Prompt | LLM | Parser).\n",
        "    #\n",
        "    PromptTemplate = ChatPromptTemplate.from_messages(\n",
        "                                                       [\n",
        "                                                         (\n",
        "                                                           \"system\",\n",
        "                                                           f\"You are an expert social media content writer for {self.platform}.\"\n",
        "                                                           f\"Your post must **STRICTLY** follow these rules: {self.rules}\"\n",
        "                                                           f\"The final output must only contain the generated content.\"\n",
        "                                                         ),\n",
        "                                                         (\n",
        "                                                           \"user\",\n",
        "                                                           \"Generate content for the following topic: {topic}. Target tone: {tone}. **Output MUST be in {language}.**\"\n",
        "                                                         ) # CHANGE: Added {language} instruction\n",
        "                                                       ]\n",
        "                                                     )\n",
        "\n",
        "    #\n",
        "    # LangChain Component Detail: | (Pipe Operator)\n",
        "    #\n",
        "    # WHAT IT DOES    : Chains together LangChain Runnable objects (Sequencing).\n",
        "    # WHY WE USE IT   : Creates a simple, readable pipeline:\n",
        "    #                   Prompt -> LLM -> OutputParser.\n",
        "    # StrOutputParser : Simply converts the LLM's text output into a standard\n",
        "    #                   Python string.\n",
        "    #\n",
        "    return PromptTemplate | llm | StrOutputParser()\n",
        "\n",
        "  #\n",
        "  # CHANGE : Added 'language' parameter to the public generate method\n",
        "  #\n",
        "  def generate(self, topic : str, tone : str, language : str) -> str:\n",
        "    #\n",
        "    #  METHOD        : generate\n",
        "    #  WHAT IT DOES  : Executes the LangChain pipeline using the provided\n",
        "    #                  topic and tone.\n",
        "    #  WHY WE USE IT : Provides a clean, standardized public interface for\n",
        "    #                  the runner to call.\n",
        "    #  RETURNS       : The generated text content as a string.\n",
        "    #\n",
        "\n",
        "    #\n",
        "    # Invoke the chain, substituting the {topic} and {tone} variables\n",
        "    # in the prompt\n",
        "    #\n",
        "    return self.chain.invoke(\n",
        "                              {\n",
        "                                \"topic\"    : topic,\n",
        "                                \"tone\"     : tone,\n",
        "                                \"language\" : language # NEW: Pass language to the prompt\n",
        "                              }\n",
        "                            )"
      ],
      "metadata": {
        "id": "0Ne9zG-3OXMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define Agents for each platform inheriting from BaseAgent. They only need to set their unique rules**"
      ],
      "metadata": {
        "id": "EHLkUk0K_mCa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Twitter Agent**"
      ],
      "metadata": {
        "id": "g4G-_NLN_swz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class twitterAgent(baseAgent):\n",
        "  #\n",
        "  # Agent for generating Twitter/X content with its specific rule\n",
        "  #\n",
        "  def __init__(self, llm : ChatGroq):\n",
        "    rules = \"A single, highly engaging, attention-grabbing Twitter/X Hook (under 280 characters).\"\n",
        "    super().__init__(llm, \"Twitter/X\", rules) # Calls the parent (baseAgent) constructor."
      ],
      "metadata": {
        "id": "mp7VM57AAvp_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Instagram Agent**"
      ],
      "metadata": {
        "id": "-T14SVY4CiBK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class instagramAgent(baseAgent):\n",
        "  #\n",
        "  # Agent for generating Instagram content with the two-paragraph rule\n",
        "  #\n",
        "  def __init__(self, llm : ChatGroq):\n",
        "    rules = \"A fun, two-paragraph caption for Instagram. Separate the paragraphs with a double line break.\"\n",
        "    super().__init__(llm, \"Instagram\", rules)"
      ],
      "metadata": {
        "id": "mDZAI6OhCkFE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Facebook Agent**"
      ],
      "metadata": {
        "id": "cr9HWmkGC9u-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class facebookAgent(baseAgent):\n",
        "  #\n",
        "  # Agent for generating Facebook content with the friendly, conversational rule\n",
        "  #\n",
        "  def __init__(self, llm : ChatGroq):\n",
        "    rules = \"A friendly, conversational post for Facebook (2-3 sentences).\"\n",
        "    super().__init__(llm, \"Facebook\", rules)"
      ],
      "metadata": {
        "id": "f99di6FADCDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **LinkedIn Agent**"
      ],
      "metadata": {
        "id": "ZN9AiTZ2Dhpm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class linkedInAgent(baseAgent):\n",
        "  #\n",
        "  # Agent for generating LinkedIn content (3-4 sentences)\n",
        "  #\n",
        "  def __init__(self, llm : ChatGroq):\n",
        "    #\n",
        "    # The conditional \"empty string\" logic is handled by the ContentRunner, not here.\n",
        "    #\n",
        "    # rules = \"A professional, insightful summary suitable for a LinkedIn post (3-4 sentences).\"\n",
        "    rules = (\n",
        "              \"Generate a professional, insightful summary suitable for a LinkedIn post (3-4 sentences).\\n\"\n",
        "              \"**Tone & Persona:** Act as a LinkedIn Content Strategist. Your voice must be **authoritative, clear, and empathetic**.\\n\"\n",
        "              \"Maintain a professional yet approachable attitude.\\n\"\n",
        "              \"**Style Constraints:** Use contractions, and strictly limit exclamation points to a maximum of one per post.\\n\"\n",
        "              \"**Vocabulary Constraints:** Avoid financial jargon, and never use the words 'disruptive' or 'guru'.\\n\"\n",
        "            )\n",
        "    # rules = (\n",
        "    #           \"You are a professional social media copywriter. Generate compelling content for the user's topic for Twitter/X, Instagram, Facebook, and LinkedIn.\\n\"\n",
        "    #           \"**LinkedIn Content Rule:** Before generating, assess if the user's topic is suitable for a professional/business audience (e.g., IT, finance, careers, company culture, technology).\\n\"\n",
        "    #           \"- If the topic IS suitable (e.g., 'Influence of AI in IT'), generate the professional summary.\\n\"\n",
        "    #           \"- If the topic is NOT suitable (e.g., 'Effects of Honey + Hot Water'), provide an **empty string (\"\")** for the 'linkedin_article_summary' field.\"\n",
        "    #         )\n",
        "\n",
        "    super().__init__(llm, \"LinkedIn\", rules)"
      ],
      "metadata": {
        "id": "g4V8IgssDl94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **HashTag Agent**"
      ],
      "metadata": {
        "id": "XdHV8qsqE8SE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# class hashTagAgent(baseAgent):\n",
        "#   #\n",
        "#   # Agent for generating exactly three highly specific hashtags\n",
        "#   #\n",
        "#   def __init__(self, llm : ChatGroq):\n",
        "#     rules = \"Exactly 3 relevant, highly specific hashtags, separated by spaces (e.g., #productivitytips #remotework #ai)\"\n",
        "#     super().__init__(llm, \"Hashtag Generator\", rules)\n",
        "\n",
        "#\n",
        "# CRITICAL FIX: Hashtag Agent with Overridden Chain\n",
        "#\n",
        "class hashTagAgent(baseAgent):\n",
        "  #\n",
        "  # Agent for generating exactly three highly specific hashtags\n",
        "  #\n",
        "  def __init__(self, llm : ChatGroq):\n",
        "    rules = \"Exactly 3 relevant, highly specific hashtags, separated by spaces (e.g., #productivitytips #remotework #ai)\"\n",
        "    super().__init__(llm, \"Hashtag Generator\", rules)\n",
        "\n",
        "  #\n",
        "  # NEW METHOD: Override baseAgent's chain creation\n",
        "  #\n",
        "  def _createChain(self, llm : ChatGroq):\n",
        "    #\n",
        "    # This new system prompt explicitly makes the agent a \"generation engine\"\n",
        "    # and uses all-caps instructions to ensure compliance.\n",
        "    #\n",
        "    promptTemplate = ChatPromptTemplate.from_messages(\n",
        "                                                       [\n",
        "                                                         (\n",
        "                                                           \"system\",\n",
        "                                                           f\"You are a specialized hashtag generation engine. Your sole task is to output a string consisting of **EXACTLY 3** relevant, highly specific hashtags, separated by spaces. Your output MUST NOT contain any other characters, descriptions, or introductory phrases. The rule is: {self.rules}\"\n",
        "                                                         ),\n",
        "                                                         (\n",
        "                                                           \"user\",\n",
        "                                                           \"Generate hashtags for the topic: {topic}. Target tone: {tone}. **Output MUST be in {language}.**\" # CHANGE: Added {language} instruction\n",
        "                                                         )\n",
        "                                                       ]\n",
        "                                                     )\n",
        "    return promptTemplate | llm | StrOutputParser()"
      ],
      "metadata": {
        "id": "SZjiE392E_vz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Content Runner Agent**\n",
        "**Content Runner (Orchestration)**"
      ],
      "metadata": {
        "id": "_KyvC809GbNs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class contentRunner:\n",
        "  #\n",
        "  # CLASS: ContentRunner\n",
        "  # WHAT IT DOES  : Acts as the central controller (the 'Runner').\n",
        "  #                 It manages the LLM instance, initializes all agents, and\n",
        "  #                 orchestrates the entire content generation workflow.\n",
        "  # WHY WE USE IT : Separates the setup/logic from the Gradio UI and centralizes\n",
        "  #                 the crucial LinkedIn conditional check.\n",
        "  #\n",
        "  def __init__(self, groqAPIKey : str):\n",
        "    #\n",
        "    # Constructor: Dtores the key and immediately sets up the LLM and Agents.\n",
        "    #\n",
        "    self.groqAPIKey = groqAPIKey\n",
        "    self.llm        = self._initializeLLM()\n",
        "    self.agents     = self._initializeAgents()\n",
        "\n",
        "  def _initializeLLM(self):\n",
        "    #\n",
        "    # Initializes the Groq LLM instance, validating the key first\n",
        "    # Raise error, if the API Key is UNKNOWN\n",
        "    #\n",
        "    if self.groqAPIKey == UNKNOWN_API_KEY:\n",
        "      #\n",
        "      # Use a ValueError to signal a critical, unrecoverable error back to\n",
        "      # the caller\n",
        "      #\n",
        "      raise ValueError(\"GROQ_API_KEY is missing. Please make sure it's set\")\n",
        "\n",
        "    #\n",
        "    # Initializes the LangChain ChatGroq wrapper\n",
        "    #\n",
        "    return ChatGroq(\n",
        "                     temperature  = 0.7,             # Controls randomness (0.0 = deterministic, 1.0 = creative)\n",
        "                     groq_api_key = self.groqAPIKey,\n",
        "                     model        = GROQ_MODEL       # Specifies the fast, chosen model\n",
        "                   )\n",
        "\n",
        "  def _initializeAgents(self):\n",
        "    #\n",
        "    # Instantiates all platform-specific agents using the initialized LLM\n",
        "    #\n",
        "    return {\n",
        "             \"Twitter\"   : twitterAgent(self.llm),\n",
        "             \"Instagram\" : instagramAgent(self.llm),\n",
        "             \"Facebook\"  : facebookAgent(self.llm),\n",
        "             \"LinkedIn\"  : linkedInAgent(self.llm),\n",
        "             \"HashTags\"  : hashTagAgent(self.llm)\n",
        "           }\n",
        "\n",
        "  #\n",
        "  # CHANGE: Added 'language' parameter\n",
        "  #\n",
        "  def runAll(self, topic : str, tone : str, language : str):\n",
        "    #\n",
        "    # METHOD        : runAll\n",
        "    # WHAT IT DOES  : The main execution method. Runs classification, then all\n",
        "    #                 agents sequentially\n",
        "    # WHY WE USE IT : This is the single entry point to generate all content\n",
        "    # RETURNS       : A tuple containing a dictionary of all results and a\n",
        "    #                 status message\n",
        "    #\n",
        "\n",
        "    #\n",
        "    # Critical Classification check\n",
        "    # (Determines if the LinkedIn Agent should run)\n",
        "    #\n",
        "    isRelevant = classifyTopic(self.llm, topic)\n",
        "    relevanceCheck = f\"Topic Relevance Check: **{isRelevant}**\"\n",
        "\n",
        "    #\n",
        "    # Sequential content generation using the agent instances\n",
        "    #\n",
        "    agentResults = {}\n",
        "\n",
        "    #\n",
        "    # Run agents for platforms without conditional rules\n",
        "    # CHANGE: Passed 'language' to all agent generation calls\n",
        "    #\n",
        "    agentResults[\"Twitter\"]   = self.agents[\"Twitter\"].generate(topic, tone, language)\n",
        "    agentResults[\"Instagram\"] = self.agents[\"Instagram\"].generate(topic, tone, language)\n",
        "    agentResults[\"Facebook\"]  = self.agents[\"Facebook\"].generate(topic, tone, language)\n",
        "    agentResults[\"HashTags\"]  = self.agents[\"HashTags\"].generate(topic, tone, language)\n",
        "\n",
        "    #\n",
        "    # Apply LinkedIn Conditional Logic (The primary rule enforcement)\n",
        "    #\n",
        "    if isRelevant == \"Yes\":\n",
        "      #\n",
        "      # CHANGE: Passed 'language' to LinkedIn generation call\n",
        "      #\n",
        "      agentResults[\"LinkedIn\"] = self.agents[\"LinkedIn\"].generate(topic, tone, language)\n",
        "    else:\n",
        "      #\n",
        "      # If not relevant, fulfill the rule by setting the output to an\n",
        "      # empty string.\n",
        "      #\n",
        "      agentResults[\"LinkedIn\"] = \"Content Skipped : This topic was automatically deemed **unsuitable for a professional LinkedIn audience**, so content generation was skipped for this platform.\"\n",
        "      relevanceCheck += \"\\n*(LinkedIn Post skipped due to non-business/tech relevance rule.)*\"\n",
        "\n",
        "    return agentResults, relevanceCheck"
      ],
      "metadata": {
        "id": "aq_3EOJ0GicX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Generate Content**"
      ],
      "metadata": {
        "id": "Igp7981KWUAD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gradio Runner Function and Setup**"
      ],
      "metadata": {
        "id": "7mMc2LRWWbS-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# CHANGE: Added 'language' parameter\n",
        "#\n",
        "def generateContent(topic : str, tone : str, language : str):\n",
        "  #\n",
        "  # FUNCTION      : generateContent\n",
        "  # WHAT IT DOES  : This function acts as the 'glue' between the Gradio UI and\n",
        "  #                 the ContentRunner logic\n",
        "  # WHY WE USE IT : Gradio requires a Python function to connect its button clicks\n",
        "  #                 to the back-end code\n",
        "  # RETURNS       : A tuple of 6 strings, which Gradio maps to the 6 output\n",
        "  #                 components\n",
        "  #\n",
        "  try:\n",
        "    #\n",
        "    # Initialize the Runner\n",
        "    # This is where the LLM is initialized and the API Key is checked\n",
        "    #\n",
        "    runner = contentRunner(API_KEY)\n",
        "\n",
        "    #\n",
        "    # Execute the main orchestration logic\n",
        "    # CHANGE: Passed 'language' to runAll\n",
        "    #\n",
        "    runResults, statusMessage = runner.runAll(topic, tone, language)\n",
        "\n",
        "    #\n",
        "    # Return results in the exact order Gradio expects\n",
        "    # (as defined in the .click() call)\n",
        "    #\n",
        "    return (\n",
        "             runResults[\"Twitter\"],\n",
        "             runResults[\"Instagram\"],\n",
        "             runResults[\"Facebook\"],\n",
        "             runResults[\"LinkedIn\"],\n",
        "             runResults[\"HashTags\"],\n",
        "             statusMessage\n",
        "           )\n",
        "  except ValueError as e:\n",
        "    #\n",
        "    # Handles the specific error raised if API_KEY is UNKNOWN\n",
        "    #\n",
        "    return (\"\",\"\",\"\",\"\",\"\", f\"‚ö†Ô∏è {str(e).replace(API_KEY, '')}\")\n",
        "  except Exception as e:\n",
        "    #\n",
        "    # Handles general errors (e.g., network issues, Groq API call failure)\n",
        "    #\n",
        "    return (\"\",\"\",\"\",\"\",\"\", f\"‚ùå Error communicating with LLM: {str(e).replace(API_KEY, '')}\")"
      ],
      "metadata": {
        "id": "PtAHY2_-Wdgy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Interface**"
      ],
      "metadata": {
        "id": "Ohl5PLJYfoT-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gradio Interface**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**The Purpose of the `with` Statement**  \n",
        "The primary goal of the `with` statement is to simplify the `try...finally` pattern, making code cleaner and safer.\n",
        "\n",
        "* **Setup (`__enter__`)**  \n",
        "When Python executes the with statement, it calls a special method on the object called `__enter__`. This method performs the necessary setup (e.g., opening a file, starting a database transaction, or initiating a UI block). The return value of this method is assigned to the variable after the `as` keyword (e.g., `as demo` or `as file`).\n",
        "\n",
        "* **Teardown (`__exit__`)**  \n",
        "Once the code block finishes‚Äîwhether normally or because an exception was raised‚ÄîPython guarantees that it will call another special method, `__exit__`. This method handles the cleanup (e.g., closing the file, committing the transaction, or finalizing the UI block).\n",
        "\n",
        "This guarantee prevents resource leaks (like an open file handle) that could lead to system instability or errors.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Example : Gradio Blocks (Structural Context)**\n",
        "\n",
        "The `with` construct in the Social Media Generator is used to define a **hierarchical structure** and **scoping** for the web interface, ensuring that UI components are correctly grouped inside their parent containers (like rows and columns).\n",
        "\n",
        "| Method | Purpose |\n",
        "| :--- | :--- |\n",
        "| `__enter__` | Sets the object as the currently active container. |\n",
        "| `__exit__` | Finalizes the container and restores the previous parent container. |\n"
      ],
      "metadata": {
        "id": "-J8Eyq9zfrNm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with gr.Blocks(\n",
        "                theme = gr.themes.Soft(),\n",
        "                title = \"(4-in-1) - Social Media Content Generator\"\n",
        "              ) as contentGenerator:\n",
        "  #\n",
        "  # gr.Blocks: The container for the whole User Interface\n",
        "  #\n",
        "  gr.Markdown(\"## ‚úçÔ∏è Multi-Agent (4-in-1) Multi-Lingual Social Media Content Generator\")\n",
        "  # gr.Markdown(\"### Using (LangChain + Groq)\")\n",
        "\n",
        "  with gr.Row(): # Groups components horizontally\n",
        "    with gr.Column(scale = 1): # Groups components vertically within the row\n",
        "      topicInput = gr.Textbox(\n",
        "                               label = \"Content Topic\",\n",
        "                               value = \"The benefits of switching to a 4-day work week for employee retention.\" # \"Why remote work is changing company culture.\"\n",
        "                             )\n",
        "\n",
        "      toneInput = gr.Textbox(\n",
        "                              label = \"Target Style/Tone\",\n",
        "                              value = \"Uplifting and professional.\"\n",
        "                            )\n",
        "\n",
        "      #\n",
        "      # NEW COMPONENT: Language Dropdown\n",
        "      #\n",
        "      languageInput = gr.Dropdown(\n",
        "                                   label   = \"Output Language\",\n",
        "                                   choices = [\"English\", \"Spanish\", \"French\", \"German\", \"Japanese\", \"Mandarin\", \"Portuguese\", \"Tamil\"], # Example supported languages\n",
        "                                   value   = \"English\" # Set a default\n",
        "                                )\n",
        "\n",
        "      generateButton = gr.Button(\"Generate Content\", variant = \"primary\")\n",
        "\n",
        "      with gr.Column(scale = 2):\n",
        "        statusOutput = gr.Markdown(\"Status : Ready. Please enter a topic, tone and language.\")\n",
        "\n",
        "  gr.Markdown(\"---\")\n",
        "  gr.Markdown(\"## üìù Generated Content\")\n",
        "\n",
        "  #\n",
        "  # Outputs are grouped by format type\n",
        "  #\n",
        "  # -----------------------------------------------------------\n",
        "  # Layout Overview: Grouping for Visual Aesthetics & Efficiency\n",
        "  # -----------------------------------------------------------\n",
        "  #\n",
        "  # Top Row: Inputs and Status\n",
        "  # --------------------------\n",
        "  # - The first gr.Row() groups:\n",
        "  #     * gr.Column(scale=1): Inputs and action button\n",
        "  #     * gr.Column(scale=2): Status / output display\n",
        "  # - This cleanly separates the control panel from the results area.\n",
        "  #\n",
        "  # Second Row: Hashtags and LinkedIn\n",
        "  # ---------------------------------\n",
        "  # - Hashtags: short, single-line text\n",
        "  # - LinkedIn Post: medium-length (approx. 4 lines)\n",
        "  # - Placing them side-by-side uses horizontal space efficiently.\n",
        "  #\n",
        "  # Third Row: Twitter and Facebook\n",
        "  # -------------------------------\n",
        "  # - Twitter/X: short (‚âà2 lines)\n",
        "  # - Facebook: medium-short (‚âà3 lines)\n",
        "  # - Grouping these quick-read outputs together improves scanability.\n",
        "  #\n",
        "  # Instagram (Standalone)\n",
        "  # ----------------------\n",
        "  # - insta_out is placed outside any gr.Row().\n",
        "  # - It's the longest output (‚âà5 lines) and benefits from full-width display.\n",
        "  # - This makes longer, multi-paragraph content easier to read.\n",
        "  #\n",
        "  # Instagram output (full-width)\n",
        "  #\n",
        "  # This grouping maximizes space efficiency and presents related content types\n",
        "  # together, making the UI easier to scan\n",
        "  #\n",
        "  with gr.Row():\n",
        "    hashTagOutput  = gr.Textbox(\n",
        "                                 label       = \"Hashtags (Exactly 3 highly specific)\",\n",
        "                                 interactive = False\n",
        "                                 # lines       = 2\n",
        "                               )\n",
        "    linkedInOutput = gr.Textbox(\n",
        "                                 label       = \"üíº LinkedIn Post (Professional, 3-4 sentences - **Skipped if not business/tech**)\",\n",
        "                                 interactive = False,\n",
        "                                 lines       = 4\n",
        "                               )\n",
        "\n",
        "  with gr.Row():\n",
        "    twitterOutput  = gr.Textbox(\n",
        "                                 label       = \"üê¶Twitter/X Hook (Single, under 280 chars)\",\n",
        "                                 interactive = False,\n",
        "                                 lines       = 2\n",
        "                               )\n",
        "    facebookOutput = gr.Textbox(\n",
        "                                 label       = \"üëç Facebook Post (Friendly, 2-3 sentences)\",\n",
        "                                 interactive = False,\n",
        "                                 lines       = 3\n",
        "                               )\n",
        "\n",
        "  instagramOutput = gr.Textbox(\n",
        "                                label       = \"üì∏ Instagram Caption (Fun, Two Paragraphs, Double Line Break)\",\n",
        "                                interactive = False,\n",
        "                                lines       = 5\n",
        "                              )\n",
        "\n",
        "  #\n",
        "  # Connect the button click event to the Python function\n",
        "  #\n",
        "  generateButton.click(\n",
        "                        fn      = generateContent, # The function to run when the button is clicked\n",
        "                        inputs  = [\n",
        "                                    topicInput,\n",
        "                                    toneInput,\n",
        "                                    languageInput # NEW: Added language input\n",
        "                                  ], # List of inputs to pass to generate_content\n",
        "                        outputs = [\n",
        "                                    twitterOutput,\n",
        "                                    instagramOutput,\n",
        "                                    facebookOutput,\n",
        "                                    linkedInOutput,\n",
        "                                    hashTagOutput,\n",
        "                                    statusOutput\n",
        "                                  ] # List of outputs to update\n",
        "                      )"
      ],
      "metadata": {
        "id": "8EgzYn3qfvsU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Launch**"
      ],
      "metadata": {
        "id": "EWMTdHwMGP_T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Possible Tones that we can use**\n",
        "\n",
        "| Tone Adjectives | Example Prompt Instruction |\n",
        "| :--- | :--- |\n",
        "| **Professional** and **Authoritative** | \"Write an analysis of the latest industry report. Use an authoritative and expert tone.\" |\n",
        "| **Friendly** and **Approachable** | \"Draft a post about my weekend project. Keep the tone casual and encouraging.\" |\n",
        "| **Inspiring** and **Visionary** | \"Generate a post on the future of work. The tone should be inspirational and forward-looking.\" |\n",
        "| **Candid** and **Humble** | \"Write about a recent business failure. Maintain a candid and humble tone, focusing on lessons learned.\" |"
      ],
      "metadata": {
        "id": "0NYnNc_yiV7l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# Launch the Content Generator\n",
        "#\n",
        "contentGenerator.launch(\n",
        "                         quiet = True,\n",
        "                         share = False # SHARE / NO_SHARE - Public Sharable Link\n",
        "                       )"
      ],
      "metadata": {
        "id": "iHhEkeSXGTw0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}